# -*- coding: utf-8 -*-
"""ass2Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WndvD0jZi_OvWZwnAobDinrQe8j_Ipkj
"""

import numpy as np
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re
from sklearn.metrics import confusion_matrix
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import ngrams
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
lm = WordNetLemmatizer()
StopWords= set(stopwords.words('english'))

CoronaTrain= np.array(pd.read_csv('/content/Corona_train.csv'))
CoronaValidation= np.array(pd.read_csv('/content/Corona_validation.csv'))
TwitterTrain1= np.array(pd.read_csv('/content/Twitter_train_1.csv'))
TwitterTrain2= np.array(pd.read_csv('/content/Twitter_train_2.csv'))
TwitterTrain5= np.array(pd.read_csv('/content/Twitter_train_5.csv'))
TwitterTrain10= np.array(pd.read_csv('/content/Twitter_train_10.csv'))
TwitterTrain25= np.array(pd.read_csv('/content/Twitter_train_25.csv'))
TwitterTrain50= np.array(pd.read_csv('/content/Twitter_train_50.csv'))
TwitterValidation= np.array(pd.read_csv('/content/Twitter_validation.csv'))

def NaiveBaiyes(sentiments,Reviews,ValidationSentiments,ValidationReviews,Ngram):
    pos,nue,neg= 0,0,0
    posdic,nuedic,negdic= {},{},{}
    postotal,nuetotal,negtotal= 0,0,0
    for i in range(len(sentiments)):
        tokens= word_tokenize(Reviews[i])
        if sentiments[i]=="Positive":
            pos+= 1
            for word in tokens:
                postotal+=1
                if word not in posdic:
                    posdic[word]=1
                else:
                    posdic[word]+=1
            if Ngram>1:
                biagram = list(nltk.bigrams(tokens))
                for word in biagram:
                    postotal+=1
                    if word not in posdic:
                        posdic[word]=1
                    else:
                        posdic[word]+=1
            if Ngram==3:
                trigram = list(ngrams(tokens, 3))
                for word in trigram:
                    postotal+=1
                    if word not in posdic:
                        posdic[word]=1
                    else:
                        posdic[word]+=1
        elif sentiments[i]=="Negative":
            neg+= 1
            for word in tokens:
                negtotal+=1
                if word not in negdic:
                    negdic[word]=1
                else:
                    negdic[word]+=1
            if Ngram>1:
                biagram = list(nltk.bigrams(tokens))
                for word in biagram:
                    negtotal+=1
                    if word not in negdic:
                        negdic[word]=1
                    else:
                        negdic[word]+=1
            if Ngram==3:
                trigram = list(ngrams(tokens, 3))
                for word in trigram:
                    negtotal+=1
                    if word not in negdic:
                        negdic[word]=1
                    else:
                        negdic[word]+=1
        else:
            nue+= 1
            for word in tokens:
                nuetotal+=1
                if word not in nuedic:
                    nuedic[word]=1
                else:
                    nuedic[word]+=1
            if Ngram>1:
                biagram = list(nltk.bigrams(tokens))
                for word in biagram:
                    nuetotal+=1
                    if word not in nuedic:
                        nuedic[word]=1
                    else:
                        nuedic[word]+=1
            if Ngram==3:
                trigram = list(ngrams(tokens, 3))
                for word in trigram:
                    nuetotal+=1
                    if word not in nuedic:
                        nuedic[word]=1
                    else:
                        nuedic[word]+=1

    ProbPos= (pos+1)/(len(sentiments)+3)
    ProbNeg= (neg+1)/(len(sentiments)+3)
    ProbNue= (nue+1)/(len(sentiments)+3)
    Prediction= np.full(ValidationReviews.shape,"Positive")
    s= set()
    for key in posdic.keys():
        s.add(key)
    for key in negdic.keys():
        s.add(key)
    for key in nuedic.keys():
        s.add(key)
    lap= len(s)

    for i in range(len(ValidationReviews)):
        LogPredPos,LogPredNeg,LogPredNue= np.log(ProbPos), np.log(ProbNeg), np.log(ProbNue)
        tokens= word_tokenize(ValidationReviews[i])
        for word in tokens:
            ProbWordGivenPos,ProbWordGivenNeg,ProbWordGivenNue= 1/(postotal+lap),1/(negtotal+lap),1/(nuetotal+lap)
            if word in posdic:
                ProbWordGivenPos= (1+posdic[word])/(postotal+lap)
            if word in negdic:
                ProbWordGivenNeg= (1+negdic[word])/(negtotal+lap)
            if word in nuedic:
                ProbWordGivenNue= (1+nuedic[word])/(nuetotal+lap)
            LogPredPos+= np.log(ProbWordGivenPos)
            LogPredNeg+= np.log(ProbWordGivenNeg)
            LogPredNue+= np.log(ProbWordGivenNue)
        if Ngram>1:
            biagram = list(nltk.bigrams(tokens))
            for word in biagram:
                ProbWordGivenPos,ProbWordGivenNeg,ProbWordGivenNue= 1/(postotal+lap),1/(negtotal+lap),1/(nuetotal+lap)
                if word in posdic:
                    ProbWordGivenPos= (1+posdic[word])/(postotal+lap)
                if word in negdic:
                    ProbWordGivenNeg= (1+negdic[word])/(negtotal+lap)
                if word in nuedic:
                    ProbWordGivenNue= (1+nuedic[word])/(nuetotal+lap)
                LogPredPos+= np.log(ProbWordGivenPos)
                LogPredNeg+= np.log(ProbWordGivenNeg)
                LogPredNue+= np.log(ProbWordGivenNue)
        if Ngram==3:
            trigram = list(ngrams(tokens, 3))
            for word in trigram:
                ProbWordGivenPos,ProbWordGivenNeg,ProbWordGivenNue= 1/(postotal+lap),1/(negtotal+lap),1/(nuetotal+lap)
                if word in posdic:
                    ProbWordGivenPos= (1+posdic[word])/(postotal+lap)
                if word in negdic:
                    ProbWordGivenNeg= (1+negdic[word])/(negtotal+lap)
                if word in nuedic:
                    ProbWordGivenNue= (1+nuedic[word])/(nuetotal+lap)
                LogPredPos+= np.log(ProbWordGivenPos)
                LogPredNeg+= np.log(ProbWordGivenNeg)
                LogPredNue+= np.log(ProbWordGivenNue)
        mx=max([LogPredPos,LogPredNeg,LogPredNue])
        if mx==LogPredNue:
            Prediction[i]="Neutral"
        elif mx==LogPredNeg:
            Prediction[i]="Negative"
    return Prediction

def CheckAccuracy(Prediction,TrueValue):
    correctPred=0
    for i in range(len(Prediction)):
        if Prediction[i]==TrueValue[i]:
            correctPred+=1
    return correctPred/len(Prediction)

PredictionOverTrain= NaiveBaiyes(CoronaTrain[:,1],CoronaTrain[:,2],CoronaTrain[:,1],CoronaTrain[:,2],1)
PredictionOverValidation= NaiveBaiyes(CoronaTrain[:,1],CoronaTrain[:,2],CoronaValidation[:,1],CoronaValidation[:,2],1)
AccuracyOverTrain= CheckAccuracy(PredictionOverTrain,CoronaTrain[:,1])
AccuracyOverValid= CheckAccuracy(PredictionOverValidation,CoronaValidation[:,1])
print(AccuracyOverTrain)
print(AccuracyOverValid)

def WordClouddd(Data):
    WholeTrainData= " ".join(Data)
    WordcloudTrain = WordCloud(width=1000, height=500, background_color='white').generate(WholeTrainData)
    plt.figure()
    plt.imshow(WordcloudTrain, interpolation= 'bilinear')
    plt.axis("off")
    plt.show()
    print()

WordClouddd(CoronaTrain[:,2])
WordClouddd(CoronaValidation[:,2])

RandomGuessOverValidation= np.full(CoronaValidation[:,1].shape,"Positive")
for i in range(len(RandomGuessOverValidation)):
    Random123=np.random.randint(1, 4)
    if Random123==1:
        RandomGuessOverValidation[i]="Positive"
    elif Random123==2:
        RandomGuessOverValidation[i]="Negative"
    else:
        RandomGuessOverValidation[i]="Neutral"

AccuracyOverRandom= CheckAccuracy(RandomGuessOverValidation,CoronaValidation[:,1])
AccuracyOverPositive= CheckAccuracy(np.full(CoronaValidation[:,1].shape,"Positive"),CoronaValidation[:,1])
print(AccuracyOverRandom)
print(AccuracyOverPositive)
ImprovmentOverRandom= (AccuracyOverValid-AccuracyOverRandom)*100
ImprovmentOverPositive= (AccuracyOverValid-AccuracyOverPositive)*100
print(ImprovmentOverRandom)
print(ImprovmentOverPositive)

def ConfusionMatrixDraw(data,prediction,titlee):
    ConfusionMatrix= confusion_matrix(data, prediction)
    plt.figure()
    plt.imshow(ConfusionMatrix, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(f"Confusion Matrix for {titlee}")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.colorbar()
    plt.xticks(np.arange(3), ["Negative", "Neutral", "Positive"], rotation=45)
    plt.yticks(np.arange(3), ["Negative", "Neutral", "Positive"])
    for i in range(3):
        for j in range(3):
            plt.text(j, i, str(ConfusionMatrix[i, j]), ha='center', va='center', color='black')
    plt.show()

ConfusionMatrixDraw(CoronaTrain[:,1], PredictionOverTrain,"Training Prediction")
ConfusionMatrixDraw(CoronaValidation[:,1], PredictionOverValidation,"Validation Prediction")
ConfusionMatrixDraw(CoronaValidation[:,1], RandomGuessOverValidation,"Random Predictions")
ConfusionMatrixDraw(CoronaValidation[:,1], np.full(CoronaValidation[:,1].shape,"Positive") ,"Positive Predictions")

def PreProcessingData(Data):
    for i in range(len(Data)):
        Links= re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',Data[i,2])
        SpecialChar= re.sub(r'[^\w\s]', '', Links)
        LowerCase= SpecialChar.lower()
        ModifiedStr= ""
        tokens= word_tokenize(LowerCase)
        for word in tokens:
            word= lm.lemmatize(word)
            if word not in StopWords:
                ModifiedStr+= word +" "
        Data[i,2]= ModifiedStr

PreProcessingData(CoronaTrain)
PreProcessingData(CoronaValidation)

WordClouddd(CoronaTrain[:,2])
WordClouddd(CoronaValidation[:,2])

PredictionNewValidation=  NaiveBaiyes(CoronaTrain[:,1],CoronaTrain[:,2],CoronaValidation[:,1],CoronaValidation[:,2],1)
AccuracyNewValidation= CheckAccuracy(PredictionNewValidation, CoronaValidation[:,1])
print(AccuracyNewValidation)

PredictionBaigram= NaiveBaiyes(CoronaTrain[:,1],CoronaTrain[:,2],CoronaValidation[:,1],CoronaValidation[:,2],2)
AccuracyBiagram= CheckAccuracy(PredictionBaigram,CoronaValidation[:,1])
print(AccuracyBiagram)

PredictionTrigram= NaiveBaiyes(CoronaTrain[:,1],CoronaTrain[:,2],CoronaValidation[:,1],CoronaValidation[:,2],3)
AccuracyTrigram= CheckAccuracy(PredictionTrigram,CoronaValidation[:,1])
print(AccuracyTrigram)

AllData= [TwitterTrain1,TwitterTrain2,TwitterTrain5,TwitterTrain10,TwitterTrain25,TwitterTrain50,TwitterValidation]

for i in range(len(AllData)):
    PreProcessingData(AllData[i])

AccuracyCombined=[]
for data in AllData:
    Prediction= NaiveBaiyes(np.append(CoronaTrain[:,1],data[:,1]),np.append(CoronaTrain[:,2],data[:,2]),data[:,1],data[:,2],1)
    AccuracyCombined.append(CheckAccuracy(Prediction,data[:,1]))

AccuracyInTarget=[]
for data in AllData:
    Prediction= NaiveBaiyes(data[:,1],data[:,2],data[:,1],data[:,2],1)
    AccuracyInTarget.append(CheckAccuracy(Prediction,data[:,1]))

print(AccuracyCombined)
print(AccuracyInTarget)
plt.figure()
plt.title('Accuracies over different Target sizes')
plt.xlabel('size')
plt.ylabel('Accuracy')
plt.plot([1,2,5,10,25,50,100],AccuracyCombined,'blue')
plt.plot([1,2,5,10,25,50,100],AccuracyInTarget,'red')
plt.show()